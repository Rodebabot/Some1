import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from imblearn.pipeline import Pipeline
import numpy as np
from sklearn.metrics import accuracy_score
import optuna

# Load your dataset
data = pd.read_csv('your_dataset.csv')

# Assuming 'success' is your target variable
target = 'success'

# Replace NaN values for categorical variables with "Not assigned"
categorical_cols = ['gender', 'other_categorical_column']  # Add your categorical column names here
data[categorical_cols] = data[categorical_cols].fillna("Not assigned")

# Replace NaN values for continuous variables with 0
continuous_cols = data.select_dtypes(include=np.number).columns.tolist()
data[continuous_cols] = data[continuous_cols].fillna(0)

# Separate features and target variable
X = data.drop(target, axis=1)
y = data[target]

# Encode categorical variables
for col in categorical_cols:
    encoder = LabelEncoder()
    X[col] = encoder.fit_transform(X[col])

# Separate categorical and continuous features
categorical_features = X[categorical_cols]
continuous_features = X.drop(columns=categorical_cols)

# Apply SMOTE to balance the classes for categorical features
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Define preprocessing steps for continuous and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), continuous_features.columns.tolist()),
        ('cat', 'passthrough', categorical_features.columns.tolist())
    ]
)

# Define the pipeline with preprocessing and Random Forest Classifier
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, log=True),
        'max_depth': trial.suggest_int('max_depth', 10, 100),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),
    }

    # Set parameters
    pipeline.set_params(classifier__n_estimators=params['n_estimators'],
                        classifier__max_depth=params['max_depth'],
                        classifier__min_samples_split=params['min_samples_split'],
                        classifier__min_samples_leaf=params['min_samples_leaf'])

    # Train the classifier
    pipeline.fit(X_train, y_train)

    # Predict on the test set
    y_pred = pipeline.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

# Perform hyperparameter optimization with Optuna
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# Get the best hyperparameters found by Optuna
best_params = study.best_params
print("Best hyperparameters found by Optuna:")
print(best_params)

# Set the best hyperparameters to the pipeline
pipeline.set_params(**best_params)

# Train the classifier with the best hyperparameters
pipeline.fit(X_train, y_train)

# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
