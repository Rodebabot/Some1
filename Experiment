import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense
from tensorflow.keras.utils import to_categorical

# Sample Data Preparation
# Assuming df is your DataFrame with 7 categorical input columns and 1 sentence output column
df = pd.DataFrame({
    'cat1': ['A', 'B', 'A', 'C'],
    'cat2': ['X', 'Y', 'Y', 'X'],
    'cat3': ['P', 'Q', 'P', 'Q'],
    'cat4': ['K', 'L', 'L', 'K'],
    'cat5': ['U', 'V', 'U', 'V'],
    'cat6': ['G', 'H', 'G', 'H'],
    'cat7': ['S', 'T', 'S', 'T'],
    'sentence': ['hello world', 'machine learning', 'deep learning', 'natural language processing']
})

# Encoding categorical variables
label_encoders = []
for col in df.columns[:-1]:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders.append(le)

X = df[df.columns[:-1]].values

# Tokenizing output sentences
tokenizer = Tokenizer()
tokenizer.fit_on_texts(df['sentence'])
sequences = tokenizer.texts_to_sequences(df['sentence'])
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

# Padding sequences
max_sequence_length = max(len(seq) for seq in sequences)
Y = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

# Define the model
input_dim = X.shape[1]
latent_dim = 256

# Encoder
encoder_inputs = Input(shape=(input_dim,))
x = Embedding(input_dim=len(np.unique(X)), output_dim=latent_dim)(encoder_inputs)
encoder = LSTM(latent_dim, return_state=True)
encoder_outputs, state_h, state_c = encoder(x)

# Decoder
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(input_dim=vocab_size, output_dim=latent_dim)
decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
decoder_dense = Dense(vocab_size, activation='softmax')

# Decoder process
decoder_inputs_embedded = decoder_embedding(decoder_inputs)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs_embedded, initial_state=[state_h, state_c])
decoder_outputs = decoder_dense(decoder_outputs)

# Compile the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy')

# Prepare data for training
decoder_input_data = np.zeros((len(X), max_sequence_length), dtype='float32')
decoder_target_data = np.zeros((len(X), max_sequence_length, vocab_size), dtype='float32')

for i, seq in enumerate(Y):
    for t, word_id in enumerate(seq):
        if t > 0:
            decoder_target_data[i, t - 1, word_id] = 1.
        decoder_input_data[i, t] = word_id

# Train the model
model.fit([X, decoder_input_data], decoder_target_data, batch_size=16, epochs=100, validation_split=0.2)

# Inference for prediction
# Encoder model
encoder_model = Model(encoder_inputs, [state_h, state_c])

# Decoder model
decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_inputs_single = Input(shape=(1,))
decoder_inputs_single_embedded = decoder_embedding(decoder_inputs_single)
decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_embedded, initial_state=[decoder_state_input_h, decoder_state_input_c])
decoder_outputs = decoder_dense(decoder_outputs)
decoder_model = Model([decoder_inputs_single] + [decoder_state_input_h, decoder_state_input_c], [decoder_outputs] + [state_h, state_c])

# Function to decode the sequence
def decode_sequence(input_seq):
    states_value = encoder_model.predict(input_seq)
    target_seq = np.zeros((1, 1))
    stop_condition = False
    decoded_sentence = ''
    while not stop_condition:
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_word = tokenizer.index_word.get(sampled_token_index, '')
        decoded_sentence += ' ' + sampled_word
        if sampled_word == '' or len(decoded_sentence.split()) > max_sequence_length:
            stop_condition = True
        target_seq = np.zeros((1, 1))
        target_seq[0, 0] = sampled_token_index
        states_value = [h, c]
    return decoded_sentence.strip()

# Example prediction
example_input = np.array([[0, 1, 0, 1, 0, 1, 0]])  # Encoded input example
decoded_sentence = decode_sequence(example_input)
print("Decoded sentence:", decoded_sentence)
