import pandas as pd
from collections import Counter
import re
from nltk.util import ngrams
from nltk.tokenize import sent_tokenize

# Ensure 'Days Spent' column is numeric
action['Days Spent'] = pd.to_numeric(action['Days Spent'], errors='coerce')

# List of specified ROCs
specified_rocs = [
    'counterpartytoaction', 
    'cptytoactioneconomicbreak', 
    'cptytoactiontemplatenegotiation'
]

# Filter the dataframe to include only the specified ROCs
filtered_action = action[action['ROC'].isin(specified_rocs)]

# Group by Counterparty Name, ROC, Business Event, and Product Representation, then calculate the median of Days Spent
median_days = filtered_action.groupby(['Counterparty Name', 'ROC', 'Business Event', 'Product Representation'])['Days Spent'].median().reset_index()

# For each specified ROC, find the top 30 counterparties with the highest median Days Spent
top_30_per_roc = pd.DataFrame()

for roc in specified_rocs:
    top_30 = median_days[median_days['ROC'] == roc].nlargest(30, 'Days Spent')
    top_30['Rank'] = top_30['Days Spent'].rank(ascending=False)
    top_30_per_roc = pd.concat([top_30_per_roc, top_30])

# Analyze the Latest Confirmation Status Comment for the top 30 counterparties per ROC
def clean_text(text):
    # Remove special characters, numbers, and extra spaces
    text = re.sub(r'[^A-Za-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_sentences(text):
    return sent_tokenize(text)

def get_ngrams(text, n):
    words = text.split()
    n_grams = ngrams(words, n)
    return [' '.join(gram) for gram in n_grams]

comments_analysis = {}

for roc in specified_rocs:
    comments = filtered_action[filtered_action['ROC'] == roc]
    top_30_counterparties = top_30_per_roc[top_30_per_roc['ROC'] == roc]['Counterparty Name'].unique()
    for counterparty in top_30_counterparties:
        relevant_comments = comments[comments['Counterparty Name'] == counterparty]['Latest Confirmation Status Comment'].dropna().apply(clean_text)
        all_phrases = []
        for comment in relevant_comments:
            sentences = extract_sentences(comment)
            for sentence in sentences:
                all_phrases.extend(get_ngrams(sentence, 4))  # Extracting 4-grams
                all_phrases.extend(get_ngrams(sentence, 5))  # Extracting 5-grams
                all_phrases.extend(get_ngrams(sentence, 6))  # Extracting 6-grams
        common_phrases = Counter(all_phrases).most_common(10)  # Change 10 to any number to get more frequent phrases
        if roc not in comments_analysis:
            comments_analysis[roc] = {}
        comments_analysis[roc][counterparty] = common_phrases

# Print the results
for roc, counterparty_phrases in comments_analysis.items():
    print(f"Top phrases for ROC {roc}:")
    for counterparty, phrases in counterparty_phrases.items():
        print(f"\nCounterparty: {counterparty}")
        for phrase, count in phrases:
            print(f"{phrase}: {count}")

# Save the results to a CSV file
results = []

for roc, counterparty_phrases in comments_analysis.items():
    for counterparty, phrases in counterparty_phrases.items():
        for phrase, count in phrases:
            results.append({'ROC': roc, 'Counterparty': counterparty, 'Phrase': phrase, 'Count': count})

results_df = pd.DataFrame(results)
results_df.to_csv('/mnt/data/comments_analysis_phrases_per_roc.csv', index=False)
