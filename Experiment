from sklearn.preprocessing import OneHotEncoder
from scipy import sparse

# Define a function to one-hot encode categorical variables into sparse matrices
def one_hot_encode_categorical_sparse(df):
    encoder = OneHotEncoder(handle_unknown='ignore', sparse=True)
    encoded_data = encoder.fit_transform(df)
    return encoded_data

# Convert non-string columns to strings
X_train = X_train.astype(str)

# Fill NaN values with a specific value (e.g., "Not Mentioned")
X_train.fillna("Not Mentioned", inplace=True)

# One-hot encode categorical variables into sparse matrices
X_train_encoded = one_hot_encode_categorical_sparse(X_train)

# Apply SMOTENC to the training set
smote_nc = SMOTENC(categorical_features=categorical_indices, random_state=42)
X_train_resampled, y_train_resampled = smote_nc.fit_resample(X_train_encoded, y_train)

# Convert sparse matrix to dense array for XGBoost training
X_train_resampled = X_train_resampled.toarray()

# Train XGBoost classifier
model = XGBClassifier()
model.fit(X_train_resampled, y_train_resampled)

# Feature importance
feature_importance = model.feature_importances_

# Create a DataFrame for feature importance
feature_importance_df = pd.DataFrame({'Feature': range(X_train_resampled.shape[1]), 'Importance': feature_importance})

# Sort the DataFrame by feature importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Calculate percentage importance
total_importance = feature_importance_df['Importance'].sum()
feature_importance_df['Percentage'] = (feature_importance_df['Importance'] / total_importance) * 100

# Display the table
print(feature_importance_df)
