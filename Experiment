import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from imblearn.pipeline import Pipeline
from sklearn.compose import make_column_selector as selector

# Load your dataset
data = pd.read_csv('your_dataset.csv')

# Assuming 'success' is your target variable
target = 'success'

# Separate features and target variable
X = data.drop(target, axis=1)
y = data[target]

# Convert known categorical columns to 'object' type
categorical_cols = ['gender', 'other_categorical_column']  # Add your categorical column names here
X_categorical = X[categorical_cols]
X_continuous = X.drop(columns=categorical_cols)

# Encode categorical variables
for col in categorical_cols:
    encoder = LabelEncoder()
    X_categorical[col] = encoder.fit_transform(X_categorical[col])

# Apply SMOTE to balance the classes for categorical features
smote = SMOTE(random_state=42)
X_categorical_resampled, y_resampled = smote.fit_resample(X_categorical, y)

# Combine resampled categorical features with continuous features
X_resampled = pd.concat([X_continuous, pd.DataFrame(X_categorical_resampled, columns=categorical_cols)], axis=1)

# Split resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Define preprocessing steps for continuous and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), selector(dtype_exclude="object")),
        ('cat', 'passthrough', selector(dtype_include="object"))
    ]
)

# Define the pipeline with preprocessing and Random Forest Classifier
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Train the classifier
pipeline.fit(X_train, y_train)

# Get feature importances
feature_importances = pipeline.named_steps['classifier'].feature_importances_

# Map feature importances to feature names
continuous_feature_names = X_continuous.columns.tolist()
categorical_feature_names = categorical_cols
feature_names = continuous_feature_names + categorical_feature_names
feature_importance_dict = dict(zip(feature_names, feature_importances))

# Sort feature importances
sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

# Print feature importances
print("Feature Importances:")
for feature, importance in sorted_feature_importances:
    print(f"{feature}: {importance:.4f}")
