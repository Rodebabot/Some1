import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from imblearn.pipeline import Pipeline
import numpy as np

# Load your dataset
data = pd.read_csv('your_dataset.csv')

# Assuming 'success' is your target variable
target = 'success'

# Replace NaN values for categorical variables with "Not assigned"
categorical_cols = ['gender', 'other_categorical_column']  # Add your categorical column names here
data[categorical_cols] = data[categorical_cols].fillna("Not assigned")

# Replace NaN values for continuous variables with 0
continuous_cols = data.select_dtypes(include=np.number).columns.tolist()
data[continuous_cols] = data[continuous_cols].fillna(0)

# Separate features and target variable
X = data.drop(target, axis=1)
y = data[target]

# Encode categorical variables
for col in categorical_cols:
    encoder = LabelEncoder()
    X[col] = encoder.fit_transform(X[col])

# Separate categorical and continuous features
categorical_features = X[categorical_cols]
continuous_features = X.drop(columns=categorical_cols)

# Apply SMOTE to balance the classes for categorical features
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split resampled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Define preprocessing steps for continuous and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), continuous_features.columns.tolist()),
        ('cat', 'passthrough', categorical_features.columns.tolist())
    ]
)

# Define the pipeline with preprocessing and Random Forest Classifier
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Train the classifier
pipeline.fit(X_train, y_train)

# Get feature importances
feature_importances = pipeline.named_steps['classifier'].feature_importances_

# Map feature importances to feature names
feature_names = continuous_features.columns.tolist() + categorical_features.columns.tolist()
feature_importance_dict = dict(zip(feature_names, feature_importances))

# Sort feature importances
sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

# Print feature importances
print("Feature Importances:")
for feature, importance in sorted_feature_importances:
    print(f"{feature}: {importance:.4f}")
